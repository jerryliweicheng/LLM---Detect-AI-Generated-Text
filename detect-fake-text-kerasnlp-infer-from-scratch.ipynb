{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6300093,"sourceType":"datasetVersion","datasetId":3623988},{"sourceId":6857742,"sourceType":"datasetVersion","datasetId":3623154},{"sourceId":6987454,"sourceType":"datasetVersion","datasetId":3947266},{"sourceId":151103719,"sourceType":"kernelVersion"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-26T21:04:06.745583Z","iopub.execute_input":"2024-02-26T21:04:06.746031Z","iopub.status.idle":"2024-02-26T21:04:06.758940Z","shell.execute_reply.started":"2024-02-26T21:04:06.746002Z","shell.execute_reply":"2024-02-26T21:04:06.758023Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/detect-fake-text-kerasnlp-tf-torch-jax-infer/__results__.html\n/kaggle/input/detect-fake-text-kerasnlp-tf-torch-jax-infer/submission.csv\n/kaggle/input/detect-fake-text-kerasnlp-tf-torch-jax-infer/__notebook__.ipynb\n/kaggle/input/detect-fake-text-kerasnlp-tf-torch-jax-infer/__output__.json\n/kaggle/input/detect-fake-text-kerasnlp-tf-torch-jax-infer/custom.css\n/kaggle/input/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm\n/kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl\n/kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl\n/kaggle/input/daigt-kerasnlp-ckpt/fold0.keras\n/kaggle/input/daigt-kerasnlp-ckpt/fold1.keras\n/kaggle/input/daigt-kerasnlp-ckpt/fold2.keras\n/kaggle/input/daigt-kerasnlp-ckpt/model.png\n/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps\n!pip install /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:06.760775Z","iopub.execute_input":"2024-02-26T21:04:06.761502Z","iopub.status.idle":"2024-02-26T21:04:10.820948Z","shell.execute_reply.started":"2024-02-26T21:04:06.761467Z","shell.execute_reply":"2024-02-26T21:04:10.819742Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl\nkeras-core is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl\nkeras-nlp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\n\nimport keras_nlp\nimport keras_core as keras \nimport keras_core.backend as K\nimport jax\nimport tensorflow as tf\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.822608Z","iopub.execute_input":"2024-02-26T21:04:10.822934Z","iopub.status.idle":"2024-02-26T21:04:10.829686Z","shell.execute_reply.started":"2024-02-26T21:04:10.822903Z","shell.execute_reply":"2024-02-26T21:04:10.828637Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasNLP:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.832300Z","iopub.execute_input":"2024-02-26T21:04:10.832573Z","iopub.status.idle":"2024-02-26T21:04:10.840947Z","shell.execute_reply.started":"2024-02-26T21:04:10.832550Z","shell.execute_reply":"2024-02-26T21:04:10.840184Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"TensorFlow: 2.15.0\nKeras: 0.1.7\nKerasNLP: 0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    verbose = 0  # Verbosity\n    device = 'GPU'  # Device\n    seed = 42  # Random seed\n    batch_size = 6  # Batch size\n    drop_remainder = True  # Drop incomplete batches\n    ckpt_dir = \"/kaggle/input/daigt-kerasnlp-ckpt\"  # Name of pretrained models\n    sequence_length = 200  # Input sequence length\n    class_names = ['real','fake']  # Class names [A, B, C, D, E]\n    num_classes = len(class_names)  # Number of classes\n    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]\n    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping\n    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.842096Z","iopub.execute_input":"2024-02-26T21:04:10.842425Z","iopub.status.idle":"2024-02-26T21:04:10.850233Z","shell.execute_reply.started":"2024-02-26T21:04:10.842394Z","shell.execute_reply":"2024-02-26T21:04:10.849343Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.851251Z","iopub.execute_input":"2024-02-26T21:04:10.851557Z","iopub.status.idle":"2024-02-26T21:04:10.889024Z","shell.execute_reply.started":"2024-02-26T21:04:10.851534Z","shell.execute_reply":"2024-02-26T21:04:10.888105Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    \"Detect and intializes GPU/TPU automatically\"\n    try:\n        # Connect to TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n        # Set TPU strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(f'> Running on TPU', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device=CFG.device\n    except:\n        # If TPU is not available, detect GPUs\n        gpus = tf.config.list_logical_devices('GPU')\n        ngpu = len(gpus)\n         # Check number of GPUs\n        if ngpu:\n            # Set GPU strategy\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            # Print GPU details\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            # If no GPUs are available, use CPU\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy()\n            device='CPU'\n    return strategy, device","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.890766Z","iopub.execute_input":"2024-02-26T21:04:10.891049Z","iopub.status.idle":"2024-02-26T21:04:10.901942Z","shell.execute_reply.started":"2024-02-26T21:04:10.891026Z","shell.execute_reply":"2024-02-26T21:04:10.900976Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"strategy, CFG.device = get_device()\nCFG.replicas = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.903001Z","iopub.execute_input":"2024-02-26T21:04:10.903246Z","iopub.status.idle":"2024-02-26T21:04:10.920423Z","shell.execute_reply.started":"2024-02-26T21:04:10.903225Z","shell.execute_reply":"2024-02-26T21:04:10.919473Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"> Running on GPU | Num of GPUs:  2\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-detect-ai-generated-text'","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.921644Z","iopub.execute_input":"2024-02-26T21:04:10.922274Z","iopub.status.idle":"2024-02-26T21:04:10.926006Z","shell.execute_reply.started":"2024-02-26T21:04:10.922242Z","shell.execute_reply":"2024-02-26T21:04:10.925104Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(f'{BASE_PATH}/test_essays.csv')  # Read CSV file into a DataFrame\n\n# Display information about the train data\nprint(\"# Test Data: {:,}\".format(len(test_df)))\nprint(\"# Sample:\")\ndisplay(test_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.930126Z","iopub.execute_input":"2024-02-26T21:04:10.930561Z","iopub.status.idle":"2024-02-26T21:04:10.946289Z","shell.execute_reply.started":"2024-02-26T21:04:10.930523Z","shell.execute_reply":"2024-02-26T21:04:10.945330Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"# Test Data: 3\n# Sample:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         id  prompt_id          text\n0  0000aaaa          2  Aaa bbb ccc.\n1  1111bbbb          3  Bbb ccc ddd.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>Aaa bbb ccc.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1111bbbb</td>\n      <td>3</td>\n      <td>Bbb ccc ddd.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"vocab_path = '/kaggle/input/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm'\ntokenizer= keras_nlp.models.DebertaV3Tokenizer(vocab_path)\npreprocessor= keras_nlp.models.DebertaV3Preprocessor(tokenizer, sequence_length=CFG.sequence_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:10.947698Z","iopub.execute_input":"2024-02-26T21:04:10.948024Z","iopub.status.idle":"2024-02-26T21:04:11.645083Z","shell.execute_reply.started":"2024-02-26T21:04:10.948000Z","shell.execute_reply":"2024-02-26T21:04:11.644031Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"outs = preprocessor(test_df.text.iloc[0])  # Process options for the first row\n\nfor k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.646311Z","iopub.execute_input":"2024-02-26T21:04:11.646630Z","iopub.status.idle":"2024-02-26T21:04:11.721219Z","shell.execute_reply.started":"2024-02-26T21:04:11.646604Z","shell.execute_reply":"2024-02-26T21:04:11.720382Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"token_ids : torch.Size([200])\npadding_mask : torch.Size([200])\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.722290Z","iopub.execute_input":"2024-02-26T21:04:11.722646Z","iopub.status.idle":"2024-02-26T21:04:11.728379Z","shell.execute_reply.started":"2024-02-26T21:04:11.722612Z","shell.execute_reply":"2024-02-26T21:04:11.727306Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Pipeline\ndef build_dataset(texts, labels=None, batch_size=32,\n                  cache=False, drop_remainder=True,\n                  augment=False, repeat=False, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n    opt = tf.data.Options()  # Create dataset options\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch\n    return ds  # Return the built dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.729766Z","iopub.execute_input":"2024-02-26T21:04:11.730425Z","iopub.status.idle":"2024-02-26T21:04:11.739673Z","shell.execute_reply.started":"2024-02-26T21:04:11.730387Z","shell.execute_reply":"2024-02-26T21:04:11.738662Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#FetchDate\ndef get_test_dataset(test_df):\n    test_texts = test_df.text.tolist()  # Extract testation texts\n    \n    # Build testation dataset\n    test_ds = build_dataset(test_texts, labels=None,\n                             batch_size=min(CFG.batch_size*CFG.replicas, len(test_df)), cache=False,\n                             shuffle=False, drop_remainder=False, repeat=False)\n    \n    return test_ds  # Return datasets and dataframes","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.740805Z","iopub.execute_input":"2024-02-26T21:04:11.741111Z","iopub.status.idle":"2024-02-26T21:04:11.750854Z","shell.execute_reply.started":"2024-02-26T21:04:11.741079Z","shell.execute_reply":"2024-02-26T21:04:11.749916Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    # Create a DebertaV3Classifier model\n    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n        CFG.preset,\n        load_weights=False,\n        preprocessor=None,\n        num_classes=1 # one output per one option, for five options total 5 outputs\n    )\n    inputs = classifier.input\n    logits = classifier(inputs)\n        \n    # Compute final output\n    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n    model = keras.Model(inputs, outputs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.751958Z","iopub.execute_input":"2024-02-26T21:04:11.752239Z","iopub.status.idle":"2024-02-26T21:04:11.764424Z","shell.execute_reply.started":"2024-02-26T21:04:11.752211Z","shell.execute_reply":"2024-02-26T21:04:11.763581Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Get the checkpoint directory and name\nckpt_dir = CFG.ckpt_dir\nckpt_name = ckpt_dir.split('/')[3]\n\n# Copy the checkpoints to a new directory in the /kaggle directory\n!cp -r {ckpt_dir} /kaggle/{ckpt_name}\n\n# List all the checkpoint paths in the new directory\nnew_ckpt_dir = f\"/kaggle/{ckpt_name}\"\nckpt_paths = glob(os.path.join(new_ckpt_dir, '*.keras'))\n\nprint(\"Total CKPT:\", len(ckpt_paths))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:04:11.765507Z","iopub.execute_input":"2024-02-26T21:04:11.765845Z","iopub.status.idle":"2024-02-26T21:05:06.361220Z","shell.execute_reply.started":"2024-02-26T21:04:11.765816Z","shell.execute_reply":"2024-02-26T21:05:06.360130Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Total CKPT: 3\n","output_type":"stream"}]},{"cell_type":"code","source":"#Prediction (Inference)\nfold_preds = np.zeros(shape = (len(test_df),), dtype = 'float32')\n\nfor ckpt_path in tqdm(ckpt_paths):\n    model = keras.models.load_model(\n        ckpt_path, \n        compile =False, )\n    test_ds = get_test_dataset(test_df)\n    preds = model.predict(\n        test_ds,\n        batch_size = min(CFG.batch_size * CFG.replicas * 2, len(test_df)),\n        verbose = 1\n    )\n    \n    fold_preds += preds.squeeze()/len(ckpt_paths)\n    del model\n    gc.collect()\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:09:22.014365Z","iopub.execute_input":"2024-02-26T21:09:22.014826Z","iopub.status.idle":"2024-02-26T21:17:08.789625Z","shell.execute_reply.started":"2024-02-26T21:09:22.014789Z","shell.execute_reply":"2024-02-26T21:17:08.788665Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1abf4a8c93d1416f856501c474db1b66"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras_core/src/trainers/trainer.py:166: UserWarning: `jit_compile` is not yet enabled for the PyTorch backend. Proceeding with `jit_compile=False`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras_core/src/saving/serialization_lib.py:713: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n  instance.compile_from_config(compile_config)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Format predictions and true answers\npred_answers = (fold_preds > 0.5).astype(int).squeeze()\n\n# Check 5 Predictions\nprint(\"# Predictions\\n\")\nfor i in range(3):\n    row = test_df.iloc[i]\n    text  = row.text\n    pred_answer = CFG.label2name[pred_answers[i]]\n    print(f\"❓ Text {i+1}:\\n{text}\\n\")\n    print(f\"🤖 Predicted: {pred_answer}\\n\")\n    print(\"-\"*90, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:20:23.537100Z","iopub.execute_input":"2024-02-26T21:20:23.537891Z","iopub.status.idle":"2024-02-26T21:20:23.545983Z","shell.execute_reply.started":"2024-02-26T21:20:23.537854Z","shell.execute_reply":"2024-02-26T21:20:23.544932Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"# Predictions\n\n❓ Text 1:\nAaa bbb ccc.\n\n🤖 Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n❓ Text 2:\nBbb ccc ddd.\n\n🤖 Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n❓ Text 3:\nCCC ddd eee.\n\n🤖 Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n","output_type":"stream"}]}]}